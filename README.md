AI Question Generation System

A two-stage Large Language Model (LLM) pipeline that converts raw educational text into structured learning content such as MCQs, Q&A, True/False, Fill-in-the-Blanks, and Summaries.

This project is designed to work efficiently with free-tier LLMs by controlling token usage, chunking large documents, and separating reasoning from generation.

ğŸš€ Key Features

ğŸ” Two-stage LLM architecture

Stage 1: Text â†’ Structured Ideas / Summary

Stage 2: Ideas â†’ Questions or Learning Content

ğŸ§  Token-aware chunking for large documents

ğŸ“š Supports multiple learning formats:

MCQ

Questionâ€“Answer

True / False

Fill-in-the-Blank

Summary

ğŸ¯ Difficulty control using Bloomâ€™s Taxonomy (Easy / Medium / Hard)

ğŸ§ª Free-tier friendly (Gemini / local LLM compatible)

ğŸ–¥ï¸ Optional Gradio-based UI

ğŸ“„ Clean JSON outputs for downstream use

ğŸ—ï¸ Architecture Overview
Raw Text
   â†“
[ Chunking + Token Control ]
   â†“
Stage 1 LLM (Idea Extraction / Summary)
   â†“
ideas.txt
   â†“
Stage 2 LLM (Question Generation)
   â†“
Structured JSON Output


âš™ï¸ How It Works
Stage 1 â€” Idea Extraction

Large documents are chunked using token limits

Each chunk is processed independently

Output is merged into a clean ideas.txt file

Stage 2 â€” Content Generation

Ideas are converted into:

MCQs

Q&A

True/False

Fill-in-the-Blanks

Summaries

Difficulty level controls cognitive depth

Output is saved as structured JSON

ğŸ–¥ï¸ Running the Gradio UI
python ui/gradio_app.py


Paste text or upload a file, select output type and difficulty, and generate learning material instantly.

ğŸ§ª Use Cases

Educational content generation

Exam preparation tools

Learning management systems (LMS)

AI tutoring systems

Automated assessment generation

ğŸ“Œ Future Improvements

Multi-language support

PDF & DOCX ingestion

Evaluation metrics for question quality

Deployment as an API

ğŸ¤ Contributing

Contributions, suggestions, and improvements are welcome!
